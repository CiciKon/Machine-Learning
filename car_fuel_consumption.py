# -*- coding: utf-8 -*-
"""car-fuel-consumption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vPTXm0_FrZEtMNj7no3w0Ily7QENloqX
"""

import pandas as pd
import numpy as np
import matplotlib
from sklearn import datasets, svm
from statistics import mean
from statistics import median
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_absolute_error

#Missing Value
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.impute import KNNImputer

#Model Selection
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import SGDRegressor
from sklearn.linear_model import ElasticNet

from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

#Metrics
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv (r'/content/drive/MyDrive/Πτυχιακή/measurements.csv')
df.head()
df.dtypes
print (df)
print(df.columns)

# apply the dtype attribute
result = df.dtypes

print("Output:")
print(result)

print(df.describe().T)

#Type Conversion for Numerical Data
df['distance'] = df['distance'].str.replace(',','.').astype('float32')
df['consume'] = df['consume'].str.replace(',','.').astype('float32')
df['temp_inside'] = df['temp_inside'].str.replace(',','.').astype('float32')
df['refill liters'] = df['refill liters'].str.replace(',','.').astype('float32')
df['specials'] = df['specials'].str.lower()

print(df.info())

print(df.isna().sum())

#temp_inside: apply imputer because low number of nulls
impute_it = IterativeImputer()
df['temp_inside'] = impute_it.fit_transform(df['temp_inside'].values.reshape(-1,1)).reshape(-1)

#specials: replace nan to other
df['specials'] = df['specials'].fillna('others')

#refill liters replace nan to 0 because not refill
df['refill liters'] = df['refill liters'].fillna(0)

#refill gas replace nan to notrefill because not refill
df['refill gas'] = df['refill gas'].fillna('norefill')

print(df.isna().sum())

features = ["specials", "gas_type", "refill gas"]

df_dummies = pd.get_dummies(df[features])
df = pd.concat([df,df_dummies], axis = 1)
df = df.drop(features + ["gas_type_SP98"], axis = 1)
df.head()

# Prediction Model Building
X = df.drop(['consume'], axis = 1)
Y = df.consume

# Split DataSet in test and train
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 8)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

models = {"Linear Regression":LinearRegression(), "SGD Regressor":SGDRegressor(), "Elastic Net":ElasticNet(), "Ada Boost Regressor":AdaBoostRegressor(), "Random Forest Regressor":RandomForestRegressor(), "Decision Tree Regressor":DecisionTreeRegressor()}
param_grids = [
    {'n_jobs': [2, 5, 7, 10], 'fit_intercept': [True, False]},
    {'alpha': [0.0001, 0.0002, 0.0005, 0.001], 'loss': ( "squared_error", "huber")},
    {'alpha': [1.0, 0.75, 0.5, 0.25]},
    {'n_estimators':[50,100,200,500,1000], 'learning_rate':[1.0, 2.0, 5.0, 10.0],'loss': ('linear', 'square')},
    {'n_estimators':[50,100,200,500], 'criterion':('squared_error', 'absolute_error', 'poisson'), 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,3]},
    {'criterion':('squared_error', 'absolute_error', 'poisson'),'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,3]}
]

from time import time

def train_grid_search(models, param_grids):
    best_params = []
    best_models = []
    best_mae = []
    best_mse = []
    best_r2 = []
    
    for idx, model_name in enumerate(models.keys()):
        t1 = time()
        print("Model Name: ", model_name)
        print("Model: ", models[model_name])
        cv = GridSearchCV(models[model_name], param_grids[idx], cv=5)
        cv.fit(X_train, y_train.values)
        
        model = cv.best_estimator_    
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test.values, y_pred)
        mse = mean_squared_error(y_test.values, y_pred)
        r2 = r2_score(y_test.values, y_pred)
        
        best_params.append(cv.best_params_)
        best_models.append(model)
        best_mae.append(mae)
        best_mse.append(mse)
        best_r2.append(r2)
        print("time:", time()-t1)
        print("----------------------------------")
    
    return {"model_names":models.keys(), "best_params":best_params, "best_models":best_models, "best_mae":best_mae, "best_mse":best_mse, "best_r2":best_r2}
results = train_grid_search(models, param_grids)

results

print("For Best Model idx","\nmae:", np.argmin(results['best_mae']), "\nmse:",np.argmin(results['best_mse']), "\nR2:",np.argmax(results['best_r2']))

best_model = RandomForestRegressor(n_estimators = 100,  criterion = 'absolute_error', min_samples_leaf= 2, min_samples_split= 5)
cv_err = cross_val_score(best_model, X_train, y_train, cv=10)

print(cv_err.mean())

# fitting the training data
model = best_model.fit(X_train,y_train)

# make predictions
y_pred = best_model.predict(X_test)
print(y_pred)

## Score
print("Score : ", model.score(X_test, y_test))
print("Training set score:", model.score(X_train, y_train))
print("Test set score:", model.score(X_test, y_test))

mae = mean_absolute_error(y_test, y_pred)
print('MAE: %.3f' % mae)